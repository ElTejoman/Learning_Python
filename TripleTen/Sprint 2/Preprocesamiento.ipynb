{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **INTRODUCCIÓN AL PREPROCESAMIENTO DE DATOS Y AL ANÁLISIS INICIAL**\n",
    "\n",
    "Como profesional de los datos, probablemente pasarás la mayor parte de tu tiempo trabajando con datos en las fases de preprocesamiento y exploración. La limpieza adecuada de los datos es fundamental para sacar conclusiones fiables de ellos.\n",
    "\n",
    "Después de estudiar este capítulo, serás capaz de utilizar diferentes métodos para:\n",
    "\n",
    "- Cambiar el nombre de las columnas.\n",
    "- Procesar valores ausentes.\n",
    "- Trabajar con duplicados.\n",
    "- Entender la agrupación.\n",
    "- Identificar las etapas de la agrupación.\n",
    "- Agrupar datos en pandas.\n",
    "- Ordenar datos para hallar valores atípicos.\n",
    "- Utilizar las características de los datos tales como valores máximos y mínimos, mediana y media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problemas con los datos: entra basura, sale basura**\n",
    "\n",
    "Los valores ausentes son valores en filas que no tenemos disponibles por alguna razón. Esto podría deberse a que la persona no respondió a una de las preguntas en una consulta, ya sea por problemas técnicos o cualquier otra razón. En los DataFrames de pandas, estos valores suelen indicarse con NaN. NaN significa \"not a number\" (\"no es un número\") y es una forma común de marcar valores ausentes.\n",
    "\n",
    "### **Errores de presentación**\n",
    "\n",
    "Es difícil determinar la cantidad de espacios visualmente, por lo que generalmente es mejor evitar usar espacios en los nombres de las columnas. Si el nombre de una columna consta de varias palabras, lo mejor es usar snake_case.\n",
    "\n",
    "Snake case (estilizado como snakecase) se refiere al estilo de escritura en el que cada espacio se reemplaza por un guion bajo () y la primera letra de cada palabra se escribe en minúsculas.\n",
    "\n",
    "Los caracteres no deseados, como los espacios, se pueden introducir de forma inesperada en los procesos de importación o exportación de datos. Esto es exactamente lo que pasó con el nombre de nuestra columna ' user_id'.\n",
    "\n",
    "Todos los temas tratados en esta lección requieren nuestra atención. Debemos abordarlos antes de proceder al análisis en sí. ¡Te enseñaremos cómo hacerlo en las próximas lecciones!\n",
    "\n",
    "### **Renombrar columnas**\n",
    "\n",
    "**Por dónde empezar**\n",
    "\n",
    "El primer paso es comprobar si realmente tienes en tus columnas un problema con la asignación de nombres. Recomendamos empezar con el método info() para obtener una idea general sobre el dataset.\n",
    "\n",
    "Recordemos cómo usarlo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método muestra no solo los nombres de las columnas, sino también información sobre los tipos de datos en la tabla y la cantidad de objetos no nulos en cada columna. Es un excelente punto de partida.\n",
    "\n",
    "Como alternativa, puedes usar el atributo .columns que solo muestra los nombres de las columnas y nada más.\n",
    "\n",
    "Para ilustrar cómo funciona el atributo .columns, creemos una tabla que contenga las distancias entre la Tierra y varios cuerpos celestes. Crearemos un DataFrame a partir de estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# las medidas se almacenan en una lista de listas\n",
    "measurements = [['Sun', 146, 152],\n",
    "                                ['Moon', 0.36, 0.41], \n",
    "                                ['Mercury', 82, 217], \n",
    "                                ['Venus', 38, 261],\n",
    "                                ['Mars', 56, 401],\n",
    "                                ['Jupiter', 588, 968],\n",
    "                                ['Saturn', 1195, 1660],\n",
    "                                ['Uranus', 2750, 3150],\n",
    "                                ['Neptune', 4300, 4700],\n",
    "                                ['Halley\\'s comet', 6, 5400]]\n",
    "\n",
    "# los nombres de las columnas se almacenan en la variable header\n",
    "header = ['Celestial bodies ','MIN', 'MAX'] \n",
    "\n",
    "# guardar el DataFrame en la variable celestial\n",
    "celestial = pd.DataFrame(data=measurements, columns=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para revisar los nombres de las columnas, vamos a mostrar el atributo columns del DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Celestial bodies ', 'MIN', 'MAX'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(celestial.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tenemos tres problemas:\n",
    "\n",
    "- 'Celestial bodies ' contiene dos espacios: entre las palabras y al final.\n",
    "- 'MIN' y 'MAX' se escriben en mayúsculas, mientras que en 'Celestial bodies ' solo se escribe con mayúscula el primer carácter. Este tipo de inconsistencia puede causar problemas.\n",
    "- Los nombres 'MIN' y 'MAX' no son muy descriptivos. Necesitamos nombres más explícitos para transmitir con claridad su significado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cambiar el nombre de las columnas, llama al método rename() con un diccionario como parámetro de columns. Las claves del diccionario deben ser los nombres anteriores de las columnas, y los valores correspondientes deben ser los nuevos nombres. De este modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['celestial_bodies', 'min_distance', 'max_distance'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   celestial_bodies  10 non-null     object \n",
      " 1   min_distance      10 non-null     float64\n",
      " 2   max_distance      10 non-null     float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 372.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Declara un diccionario con el nombre anterior de la columna como claves\n",
    "# y los nombres nuevos de la columna como los valores\n",
    "columns_new ={\n",
    "    \"Celestial bodies \": \"celestial_bodies\",\n",
    "    \"MIN\": \"min_distance\",\n",
    "    \"MAX\": \"max_distance\",\n",
    "    }\n",
    "\n",
    "# Llama al método rename y pasa\n",
    "# el diccionario como un argumento al parámetro columns\n",
    "celestial = celestial.rename(columns = columns_new)\n",
    "print(celestial.columns)\n",
    "print(celestial.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes te mostramos cómo cambiar los nombres de las columnas y reasignar la variable celestial vapara reflejar los cambios. Si no reasignas la variable, los nombres de las columnas no cambiarán.\n",
    "\n",
    "Sin embargo, hay una forma más elegante de renombrar columnas que no requiere reasignación como hicimos anteriormente. Solo necesitamos especificar el parámetro inplace y establecerlo en True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['celestial_bodies', 'min_distance', 'max_distance'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Declara un diccionario con el nombre anterior de la columna como claves\n",
    "# y los nombres nuevos de la columna como los valores\n",
    "columns_new ={\n",
    "    \"Celestial bodies \": \"celestial_bodies\",\n",
    "    \"MIN\": \"min_distance\",\n",
    "    \"MAX\": \"max_distance\",\n",
    "    }\n",
    "\n",
    "# Llama al método rename y pasa\n",
    "# el diccionario como un argumento al parámetro columns\n",
    "# y True como un argumento al parámetro inplace\n",
    "celestial.rename(columns = columns_new, inplace = True)\n",
    "print(celestial.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EJERCICIOS**\n",
    "\n",
    "Primero, debes ver si algo está mal con los nombres de las columnas y qué es. Así que comienza por mostrar los nombres de columna de la tabla df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "print(df.columns)\n",
    "#print(df)\n",
    "# escribe tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debes identificar tres problemas en los nombres de las columnas '  user_id', 'total play' y 'Artist'. Adelante, corrígelos.\n",
    "\n",
    "Renombra las siguientes tres columnas en df:\n",
    "\n",
    "- '  user_id' → 'user_id'\n",
    "- 'total play' → 'total_play'\n",
    "- 'Artist' → 'artist'\n",
    "\n",
    "Crea un diccionario con los nombres antiguos y los nuevos, y después llama al método rename() en df y pasa a este tu diccionario.\n",
    "\n",
    "En el diccionario, utiliza los nombres de columna anteriores como claves y los nuevos como valores.\n",
    "\n",
    "Luego, muestra el atributo columns para df para confirmar que los cambios se han aplicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "columns_new = {\n",
    "    \"  user_id\":\"user_id\",\n",
    "    \"total play\":\"total_play\",\n",
    "    \"Artist\":\"artist\",\n",
    "}\n",
    "\n",
    "df.rename(columns = columns_new , inplace = True)\n",
    "\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Forma automatizada de renombrar columnas**\n",
    "\n",
    "A veces, la cantidad de columnas en un dataset puede ser grande, por lo que es poco práctico asignar manualmente nuevos valores a los nombres de las columnas. Y de hecho, podría ser difícil ver los problemas en un nombre de columna. En estos casos, los bucles y los métodos de string pueden ser muy útiles.\n",
    " \n",
    "Echa un vistazo al siguiente fragmento de código que itera sobre los antiguos nombres de la columna, cámbialos y guarda los resultados en la lista new_col_names, que más tarde se asigna como los nuevos nombres de columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = []\n",
    "\n",
    "for old_name in celestial.columns:\n",
    "    # Primero, elimina los espacios al principio y al final\n",
    "    name_stripped = old_name.strip()\n",
    "    # Luego, pon todas las letras en minúsculas\n",
    "    name_lowered = name_stripped.lower()\n",
    "    # Por último, reemplaza los espacios entre palabras por guiones bajos\n",
    "    name_no_spaces = name_lowered.replace(' ', '_')\n",
    "    # Agrega el nuevo nombre a la lista de nuevos nombres de columna\n",
    "    new_col_names.append(name_no_spaces)\n",
    "\n",
    "# Reemplaza los nombres anteriores por los nuevos\n",
    "celestial.columns = new_col_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado, obtenemos un DataFrame en el que todos los nombres de las columnas se ajustan a nuestro formato deseado.\n",
    "\n",
    "¡Es muy claro! Recorre los nombres de tus columnas; elimina todos los espacios iniciales y finales usando el método strip(); haz que todo esté en minúsculas con el método lower(); remplaza cualquier espacio entre palabras con guiones bajos aplicando el método replace(); y después agrega el nuevo nombre (más claro) a la nueva lista.\n",
    "\n",
    "Ahora es tu turno nuevamente. Volvamos a nuestro dataset music_log_raw.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EJERCICIOS!!**\n",
    "\n",
    "Ahora queremos que hagas el mismo cambio de nombre, pero usando 3 métodos de string: strip(), lower() y replace(). Coloca los nuevos nombres de columna en la lista new_col_names.\n",
    "\n",
    "Luego, muestra el atributo columns para df para confirmar que los cambios se han aplicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "new_col_names = []\n",
    "\n",
    "for i in df:\n",
    "    stripped = i.strip()\n",
    "    lower = stripped.lower()\n",
    "    spaces= lower.replace(\" \",\"_\")\n",
    "    new_col_names.append(spaces)\n",
    "\n",
    "df.columns = new_col_names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Procesar valores ausentes**\n",
    "\n",
    "Ahora que hemos corregido los nombres de las columnas, podemos mostrarte cómo preprocesar los valores ausentes en los propios datos. Al final de esta lección, podrás\n",
    "\n",
    "comprobar rápidamente qué columnas tienen valores ausentes usando el método **isna()**,\n",
    " \n",
    "completar los valores ausentes con el método ***fillna()** y \n",
    "\n",
    "eliminar filas o columnas con valores ausentes utilizando el método **dropna()**.\n",
    "\n",
    "**Buscar valores ausentes**\n",
    "\n",
    "Para encontrar todos los valores ausentes en una tabla, puedes utilizar el método \n",
    "isna(). Funciona de manera bastante sencilla: si se encuentra un valor ausente, devuelve True; si no, devuelve False.\n",
    "\n",
    "isna() no es tan útil por sí solo. Generalmente usamos el método isna() junto con el método sum(). La función sum() cuenta todos los valores True y devuelve su suma total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cholera.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sustituir valores**\n",
    "\n",
    "Para conservar todas las filas con datos valiosos, reemplazaremos los valores NaN en la columna 'imported_cases' por ceros.\n",
    "\n",
    "Podemos lograr esto utilizando el método fillna(), que devuelve una copia de la columna original con todos los valores NaN reemplazados por un valor específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cholera['imported_cases'] = cholera['imported_cases'].fillna(0)\n",
    "\n",
    "print(cholera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna 'imported_cases' ahora tiene todos sus valores ausentes reemplazados por ceros. De manera alternativa, podrías haber establecido el argumento inplace=True para que no tuvieras que asignar una nueva columna en lugar de la antigua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cholera['imported_cases'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por cierto, incluso puedes usar aquí el bucle for para remplazar los valores ausentes. Todo lo que necesitas es crear una lista que contenga todas las columnas en donde quieres hacer el remplazo, y después iterar sobre esos nombres para hacer realmente el cambio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recorrer nombres de columna y remplazar los valores ausentes con ceros\n",
    "columns_to_replace = ['imported_cases']\n",
    "\n",
    "for col in columns_to_replace:\n",
    "    cholera[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EJERCICIOS!**\n",
    "\n",
    "Escribe código que sume la cantidad de valores ausentes en todas las columnas del dataset. Guarda el resultado en la variable mis_val y muéstralo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "mis_val = df.isna().sum()\n",
    "\n",
    "print(mis_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eliminar filas**\n",
    "\n",
    "Para eliminar filas con valores ausentes en un DataFrame de pandas, usa el método dropna(). Este método elimina las filas con al menos un valor ausente. También puedes especificar una lista de columnas en su parámetro subset= para que elimine filas con valores nulos solo en esas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cholera = cholera.dropna(subset=['total_cases', 'deaths', 'case_fatality_rate'])\n",
    "print(cholera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora eliminemos toda la columna 'notes', que consiste casi en su totalidad en valores ausentes.\n",
    "\n",
    "Usaremos el método dropna() de nuevo, pero esta vez agregaremos otro argumento: axis=. Este argumento nos permite especificar si queremos eliminar filas o columnas. Si pasamos el string 'columns' a axis=, eliminará las columnas que tengan valores ausentes. Dado que 'notes' es la única columna que contiene valores ausentes, podemos usar esta opción de forma segura para eliminarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cholera = cholera.dropna(axis='columns')\n",
    "print(cholera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debes saber que si tienes varias columnas con valores ausentes, cholera.dropna(axis='columns') las eliminará todas. No es siempre lo que queremos. En su lugar, puedes usar el método drop() para controlar qué columnas quieres eliminar. Esto es lo que debes hacer si solo quieres eliminar la columna 'notes' utilizando el método drop():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cholera = cholera.drop(labels=['notes'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "**Ejercicio 2**\n",
    "\n",
    "Escribe código para recorrer las columnas genre, Artist y track del DataFrame df y reemplaza cualquier valor ausente con el string 'no_info'. La lista de columnas a reemplazar se almacena en la variable columns_to_replace.\n",
    "\n",
    "Después de realizar los reemplazos, comprueba la cantidad de valores ausentes nuevamente usando isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "columns_to_replace = ['genre', 'Artist', 'track']\n",
    "\n",
    "for col in columns_to_replace:\n",
    "    df[col].fillna(\"no_info\", inplace = True)\n",
    "\t\n",
    "\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3**\n",
    "\n",
    "Ahora, eliminemos los NaNs en la columna total play remplazándolos con 0.\n",
    "Después de realizar los reemplazos, comprueba la cantidad de valores ausentes nuevamente usando isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "df[\"total play\"].fillna(0,inplace = True)\n",
    "\n",
    "\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PROCESAMIENTO DE VALORES DUPLICADOS**\n",
    "\n",
    "**duplicated()** ---> devuelve True si se duplica un valor y False en caso contrario.\n",
    "\n",
    "**drop_duplicates()** ----> Para eliminar filas completamente duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "\n",
    "**Ejercicio 1**\n",
    "\n",
    "En el fragmento de código a continuación, encontrarás la variable pop que almacena un DataFrame filtrado que contiene solo canciones pop. Tu objetivo es determinar la cantidad de duplicados en este DataFrame y almacenar este valor en la variable duplicates. Por último, muestra esta variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "pop = df[df['genre'] == 'pop']\n",
    "\n",
    "duplicates = pop.duplicated().sum()\n",
    "\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eliminación de duplicados**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, podemos volver a especificar inplace=True para que no haya necesidad de reasignación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando eliminas filas, a menudo también es importante actualizar el índice. Para hacerlo, llama al método reset_index(). Esto creará un nuevo DataFrame en el que:\n",
    "\n",
    "- Los índices del DataFrame original se ubicarán en una nueva columna llamada 'index'.\n",
    "- Los nuevos índices se establecerán en orden para todas las filas en el DataFrame.\n",
    "\n",
    "Así es como restablecemos los índices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULTADO\n",
    "\n",
    "#     index   user_id  total play                                  Artist   \n",
    "#0      0  BF6EA5AF   92.851388                              Marina Rei  \\\n",
    "#1      1  FB1E568E  282.981000                            Stive Morgan   \n",
    "#2      3  EF15C7BA    8.966000                                 no_info   \n",
    "#3      4  82F52E69  193.776327                                  Rixton   \n",
    "#4      5  4166D680    3.007000  Henry Hall & His Gleneagles Hotel Band   \n",
    "\n",
    "#     genre                   track  \n",
    "#0      pop                  Musica  \n",
    "#1  ambient             Love Planet  \n",
    "#2    dance     Loving Every Minute  \n",
    "#3      pop  Me And My Broken Heart  \n",
    "#4     jazz                    Home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado, obtuvimos la enumeración correcta para nuestras filas y también para la columna 'index', que básicamente almacena los índices anteriores. Por lo general, queremos eliminar esta columna 'index'. Para ello, necesitamos establecer el parámetro drop= en True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULTADO\n",
    "\n",
    "\n",
    "#    user_id   total play                                  Artist    genre   \n",
    "#0  BF6EA5AF   92.851388                              Marina Rei      pop  \\\n",
    "#1  FB1E568E  282.981000                            Stive Morgan  ambient   \n",
    "#2  EF15C7BA    8.966000                                     NaN    dance   \n",
    "#3  82F52E69  193.776327                                  Rixton      pop   \n",
    "#4  4166D680    3.007000  Henry Hall & His Gleneagles Hotel Band     jazz   \n",
    "\n",
    "#                    track  \n",
    "#0                  Musica  \n",
    "#1             Love Planet  \n",
    "#2     Loving Every Minute  \n",
    "#3  Me And My Broken Heart  \n",
    "#4                    Home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2**\n",
    "\n",
    "Partiendo de nuestro ejercicio anterior, debemos eliminar las filas duplicadas del DataFrame pop con el que estamos trabajando. El DataFrame resultante debe almacenarse en la misma variable pop como antes. Después de la eliminación, vuelve a comprobar el número de duplicados e imprime este número."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "pop = df[df['genre'] == 'pop']\n",
    "\n",
    "pop.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "print(pop.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Detección de duplicados implícitos**\n",
    "\n",
    "Para ver todos los valores únicos en una columna, utiliza el método unique(). Este método devuelve todos los valores únicos en una columna especificada. Así es como lo usamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date                 name  points\n",
      "0   2018.01.01         Rafael Nadal   10645\n",
      "1   2018.01.08         Rafael Nadal   10600\n",
      "2   2018.01.29         Rafael Nadal    9760\n",
      "3   2018.02.19        Roger Federer   10105\n",
      "4   2018.03.05        Roger Federer   10060\n",
      "5   2018.03.19       Roger Federerr    9660\n",
      "6   2018.04.02  Rafael Nadal Parera    8770\n",
      "7   2018.06.18         Roger Fedrer    8920\n",
      "8   2018.06.25  Rafael Nadal Parera    8770\n",
      "9   2018.07.16  Rafael Nadal Parera    9310\n",
      "10  2018.08.13  Rafael Nadal Parera   10220\n",
      "11  2018.08.20  Rafael Nadal Parera   10040\n",
      "12  2018.09.10  Rafael Nadal Parera    8760\n",
      "13  2018.10.08  Rafael Nadal Parera    8260\n",
      "14  2018.10.15  Rafael Nadal Parera    7660\n",
      "15  2018.11.05       Novak Djokovic    8045\n",
      "16  2018.11.19       Novak Djokovic    9045\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rating = ['date', 'name', 'points']\n",
    "players = [\n",
    "        ['2018.01.01',  'Rafael Nadal', 10645],\n",
    "                ['2018.01.08',  'Rafael Nadal', 10600],\n",
    "                ['2018.01.29',  'Rafael Nadal', 9760],\n",
    "                ['2018.02.19',  'Roger Federer', 10105], \n",
    "                ['2018.03.05',  'Roger Federer', 10060],\n",
    "                ['2018.03.19',  'Roger Federerr', 9660],\n",
    "                ['2018.04.02',  'Rafael Nadal Parera', 8770],\n",
    "                ['2018.06.18',  'Roger Fedrer', 8920],\n",
    "                ['2018.06.25',  'Rafael Nadal Parera', 8770],\n",
    "                ['2018.07.16',  'Rafael Nadal Parera', 9310],\n",
    "                ['2018.08.13',  'Rafael Nadal Parera', 10220],\n",
    "                ['2018.08.20',  'Rafael Nadal Parera', 10040],\n",
    "                ['2018.09.10',  'Rafael Nadal Parera', 8760],\n",
    "                ['2018.10.08',  'Rafael Nadal Parera', 8260],\n",
    "                ['2018.10.15',  'Rafael Nadal Parera', 7660],\n",
    "                ['2018.11.05',  'Novak Djokovic', 8045],\n",
    "                ['2018.11.19',  'Novak Djokovic', 9045]\n",
    "]\n",
    "tennis = pd.DataFrame(data=players, columns=rating)\n",
    "print(tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces, solo queremos saber el número de valores únicos en una columna en lugar de los valores en sí. En ese caso, podemos usar el método nunique():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rafael Nadal' 'Roger Federer' 'Roger Federerr' 'Rafael Nadal Parera'\n",
      " 'Roger Fedrer' 'Novak Djokovic']\n"
     ]
    }
   ],
   "source": [
    "print(tennis['name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(tennis['name'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eliminación de duplicados implícitos**\n",
    "\n",
    "Utiliza el método replace() para corregir la ortografía incorrecta o alternativa. Pasa el valor no deseado de la tabla como primer argumento y el valor correcto como segundo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date                 name  points\n",
      "0   2018.01.01  Rafael Nadal Parera   10645\n",
      "1   2018.01.08  Rafael Nadal Parera   10600\n",
      "2   2018.01.29  Rafael Nadal Parera    9760\n",
      "3   2018.02.19        Roger Federer   10105\n",
      "4   2018.03.05        Roger Federer   10060\n",
      "5   2018.03.19        Roger Federer    9660\n",
      "6   2018.04.02  Rafael Nadal Parera    8770\n",
      "7   2018.06.18        Roger Federer    8920\n",
      "8   2018.06.25  Rafael Nadal Parera    8770\n",
      "9   2018.07.16  Rafael Nadal Parera    9310\n",
      "10  2018.08.13  Rafael Nadal Parera   10220\n",
      "11  2018.08.20  Rafael Nadal Parera   10040\n",
      "12  2018.09.10  Rafael Nadal Parera    8760\n",
      "13  2018.10.08  Rafael Nadal Parera    8260\n",
      "14  2018.10.15  Rafael Nadal Parera    7660\n",
      "15  2018.11.05       Novak Djokovic    8045\n",
      "16  2018.11.19       Novak Djokovic    9045\n"
     ]
    }
   ],
   "source": [
    "tennis['name'] = tennis['name'].replace('Roger Federerr', 'Roger Federer')\n",
    "tennis['name'] = tennis['name'].replace('Roger Fedrer', 'Roger Federer')\n",
    "tennis['name'] = tennis['name'].replace('Rafael Nadal', 'Rafael Nadal Parera')\n",
    "\n",
    "print(tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuvimos que llamar al método replace() dos veces. Si hubiéramos tenido más faltas de ortografía, habríamos tenido que volver a llamarlo.\n",
    "\n",
    "Como siempre, pasar inplace=True produce el mismo resultado sin necesidad de reasignación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date                 name  points\n",
      "0   2018.01.01  Rafael Nadal Parera   10645\n",
      "1   2018.01.08  Rafael Nadal Parera   10600\n",
      "2   2018.01.29  Rafael Nadal Parera    9760\n",
      "3   2018.02.19        Roger Federer   10105\n",
      "4   2018.03.05        Roger Federer   10060\n",
      "5   2018.03.19        Roger Federer    9660\n",
      "6   2018.04.02  Rafael Nadal Parera    8770\n",
      "7   2018.06.18        Roger Federer    8920\n",
      "8   2018.06.25  Rafael Nadal Parera    8770\n",
      "9   2018.07.16  Rafael Nadal Parera    9310\n",
      "10  2018.08.13  Rafael Nadal Parera   10220\n",
      "11  2018.08.20  Rafael Nadal Parera   10040\n",
      "12  2018.09.10  Rafael Nadal Parera    8760\n",
      "13  2018.10.08  Rafael Nadal Parera    8260\n",
      "14  2018.10.15  Rafael Nadal Parera    7660\n",
      "15  2018.11.05       Novak Djokovic    8045\n",
      "16  2018.11.19       Novak Djokovic    9045\n"
     ]
    }
   ],
   "source": [
    "tennis['name'].replace('Roger Federerr', 'Roger Federer', inplace = True)\n",
    "tennis['name'].replace('Roger Fedrer', 'Roger Federer', inplace = True)\n",
    "tennis['name'].replace('Rafael Nadal', 'Rafael Nadal Parera', inplace = True)\n",
    "\n",
    "print(tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Automatización con funciones personalizadas**\n",
    "\n",
    "Para evitar repetir el mismo código varias veces, los profesionales de los datos suelen escribir sus propias funciones. Vamos a crear una función que tome cuatro argumentos:\n",
    "\n",
    "- el DataFrame;\n",
    "- el nombre de la columna donde queremos realizar el reemplazo;\n",
    "- una lista de valores incorrectos;\n",
    "- el valor correcto.\n",
    "\n",
    "La función reemplazará todos los valores incorrectos por el correcto en la columna seleccionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date                 name  points\n",
      "0   2018.01.01  Rafael Nadal Parera   10645\n",
      "1   2018.01.08  Rafael Nadal Parera   10600\n",
      "2   2018.01.29  Rafael Nadal Parera    9760\n",
      "3   2018.02.19        Roger Federer   10105\n",
      "4   2018.03.05        Roger Federer   10060\n",
      "5   2018.03.19        Roger Federer    9660\n",
      "6   2018.04.02  Rafael Nadal Parera    8770\n",
      "7   2018.06.18        Roger Federer    8920\n",
      "8   2018.06.25  Rafael Nadal Parera    8770\n",
      "9   2018.07.16  Rafael Nadal Parera    9310\n",
      "10  2018.08.13  Rafael Nadal Parera   10220\n",
      "11  2018.08.20  Rafael Nadal Parera   10040\n",
      "12  2018.09.10  Rafael Nadal Parera    8760\n",
      "13  2018.10.08  Rafael Nadal Parera    8260\n",
      "14  2018.10.15  Rafael Nadal Parera    7660\n",
      "15  2018.11.05       Novak Djokovic    8045\n",
      "16  2018.11.19       Novak Djokovic    9045\n"
     ]
    }
   ],
   "source": [
    "def replace_wrong_values(df, column, wrong_values, correct_value): # pasar una lista de valores incorrectos y un string con el valor correcto en la entrada de la función\n",
    "    for wrong_value in wrong_values: # looping over misspelled names\n",
    "        df[column] = df[column].replace(wrong_value, correct_value) # llamar a replace() para cada nombre incorrecto\n",
    "    return df # devolver el DataFrame modificado\n",
    "\n",
    "duplicates = ['Roger Federerr', 'Roger Fedrer'] # una lista de nombres mal escritos\n",
    "name = 'Roger Federer' # el nombre correcto\n",
    "tennis = replace_wrong_values(tennis, 'name', duplicates, name) # llamar a la función, replace() se llamará dos veces\n",
    "print(tennis) # el nuevo DataFrame sin duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3**\n",
    "\n",
    "Y finalmente, verifica el número de valores únicos en la columna 'Artist'. Guarda los valores únicos en la variable pop_artists. El número de artistas únicos debe almacenarse en la variable n_artists. Muestra ambas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_raw.csv')\n",
    "\n",
    "pop = df[df['genre'] == 'pop']\n",
    "\n",
    "pop_artists = pop['Artist'].unique()  # Obteniendo los artistas únicos\n",
    "n_artists = pop['Artist'].nunique()  # Calculando la cantidad de artistas únicos usando nunique()\n",
    "\n",
    "print(pop_artists)  # Muestra los artistas únicos\n",
    "print(\"Número de artistas únicos en la columna 'Artist':\", n_artists)  # Muestra la cantidad de artistas únicos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AGRUPACIÓN DE DATOS**\n",
    "\n",
    "La agrupación se justifica cuando los datos caen lógicamente en grupos en función de una determinada característica y cuando los grupos son relevantes para la tarea en cuestión.\n",
    "\n",
    "Por ejemplo, si tenemos datos sobre todos los artículos comprados en una tienda específica, podemos agrupar los datos por hora del día para identificar el tráfico máximo. O podríamos agrupar todas las compras por ID de cliente para calcular el tamaño de compra promedio, que es una métrica clave en el comercio minorista.\n",
    "\n",
    "**Etapas de la agrupación**\n",
    "\n",
    "- Dividir. Primero, divide los datos en grupos según un criterio determinado.\n",
    "- Aplicar. A continuación, aplica métodos de cálculo a cada grupo, por ejemplo, puedes encontrar el número de elementos en un grupo con el método count() o la suma de sus valores con sum().\n",
    "- Combinar. Finalmente, los resultados son almacenados en una nueva estructura de datos: un DataFrame o un Series.\n",
    "\n",
    "Estas son las etapas estándar de agrupación y, afortunadamente para nosotros, pandas tiene métodos integrados para ellas.\n",
    "\n",
    "**EJEMPLO**\n",
    "\n",
    "Analicemos algunos datos sobre exoplanetas para ver cómo funciona la agrupación en la práctica.\n",
    "Los científicos y las científicas ya han encontrado miles de estos planetas fuera de nuestro sistema solar utilizando telescopios en el espacio para enviar imágenes que luego son estudiadas por analistas de datos. Te mostraremos cómo encuentran planetas similares a la Tierra.\n",
    "La tabla exoplanet almacena datos sobre miles de exoplanetas. Echa un vistazo a las primeras 30 filas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/exoplanets.csv')\n",
    "\n",
    "print(exoplanet.head(30))\n",
    "\n",
    "#RESULTADO\n",
    "\n",
    "#            name      mass  radius  discovered\n",
    "#0    1RXS 1609 b  14.00000  1.7000        2008\n",
    "#1   2M 0122-24 b  20.00000  1.0000        2013\n",
    "#2   2M 2140+16 b  20.00000  0.9200        2010\n",
    "#3   2M 2206-20 b  30.00000  1.3000        2010\n",
    "#4       51 Peg b   0.47000  1.9000        1995\n",
    "#5       55 Cnc e   0.02703  0.1737        2004\n",
    "#6       CT Cha b  17.00000  2.2000        2008\n",
    "#7      CoRoT-1 b   1.03000  1.4900        2007\n",
    "#8     CoRoT-10 b   2.75000  0.9700        2010\n",
    "#9     CoRoT-11 b   2.33000  1.4300        2010\n",
    "#10    CoRoT-12 b   0.91700  1.4400        2010\n",
    "#11    CoRoT-13 b   1.30800  0.8850        2010\n",
    "#12    CoRoT-14 b   7.60000  1.0900        2010\n",
    "#13    CoRoT-15 b  63.40000  1.1200        2010\n",
    "#14    CoRoT-16 b   0.53500  1.1700        2010\n",
    "#15    CoRoT-17 b   2.43000  1.0200        2010\n",
    "#16    CoRoT-18 b   3.47000  1.3100        2011\n",
    "#17    CoRoT-19 b   1.11000  1.2900        2011\n",
    "#18     CoRoT-2 b   3.31000  1.4650        2007\n",
    "#19    CoRoT-20 b   4.24000  0.8400        2011\n",
    "#20    CoRoT-21 b   2.26000  1.3000        2011\n",
    "#21    CoRoT-22 b   0.06000  0.4354        2011\n",
    "#22    CoRoT-23 b   2.80000  1.0800        2011\n",
    "#23    CoRoT-24 b   0.01800  0.3300        2011\n",
    "#24    CoRoT-24 c   0.08800  0.4400        2011\n",
    "#25    CoRoT-25 b   0.27000  1.0800        2012\n",
    "#26    CoRoT-26 b   0.52000  1.2600        2012\n",
    "#27    CoRoT-27 b  10.39000  1.0070        2012\n",
    "#28    CoRoT-29 b   0.85000  0.9000        2012\n",
    "#29     CoRoT-3 b  21.66000  1.1900        2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Primero, dividimos los datos en grupos por año.\n",
    "- Luego, aplicamos el método count() para encontrar el número de elementos en cada grupo.\n",
    "- Por último, guardamos el resultado como una nueva tabla en la que cada fila contiene un año y el número de exoplanetas descubiertos.\n",
    "\n",
    "A continuación, verás cómo se ve esto en el código.\n",
    "\n",
    "### **Agrupación en pandas**\n",
    "\n",
    "En pandas, agrupamos los datos utilizando el método groupby(), que hace lo siguiente:\n",
    "\n",
    "- Toma el nombre de una columna en la que se deben agrupar los datos como argumento. Este argumento se llama by=. En nuestro caso, vamos a agrupar los datos por año de descubrimiento.\n",
    "- Devuelve un objeto de un tipo especial: DataFrameGroupBy. Son datos agrupados. Si les aplicas un método de pandas, se convertirán en una nueva estructura de datos de tipo DataFrame o Series.\n",
    "\n",
    "Encontremos el número de exoplanetas agrupados por año utilizando el método "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exoplanet.groupby(by='discovered'))\n",
    "print() # nos dará una línea vacía entre dos impresiones\n",
    "print(exoplanet.groupby(by='discovered').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <pandas.core.groupby.DataFrameGroupBy object at 0x7fc1e1ca3400>\n",
    "\n",
    "#           name  mass  radius\n",
    "#discovered                    \n",
    "#1995           1     1       1\n",
    "#1996           1     1       1\n",
    "#1999           1     1       1\n",
    "#2000           2     2       2\n",
    "#2001           1     1       1\n",
    "#2002           1     1       1\n",
    "#2004           7     7       7\n",
    "#2005           4     4       4\n",
    "#2006          10    10      10\n",
    "#2007          19    19      19\n",
    "#2008          23    23      23\n",
    "#2009          15    15      15\n",
    "#2010          57    57      57\n",
    "#2011          95    95      95\n",
    "#2012          73    73      73\n",
    "#2013          96    96      96\n",
    "#2014         105   105     105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No siempre es necesario especificar el argumento by=. Pasar el nombre de la columna funcionará exactamente de la misma manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exoplanet.groupby('discovered'))\n",
    "print() # nos dará una línea vacía entre dos impresiones\n",
    "print(exoplanet.groupby('discovered').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <pandas.core.groupby.DataFrameGroupBy object at 0x7fc1e1ca3400>\n",
    "\n",
    "#           name  mass  radius\n",
    "#discovered                    \n",
    "#1995           1     1       1\n",
    "#1996           1     1       1\n",
    "#1999           1     1       1\n",
    "#2000           2     2       2\n",
    "#2001           1     1       1\n",
    "#2002           1     1       1\n",
    "#2004           7     7       7\n",
    "#2005           4     4       4\n",
    "#2006          10    10      10\n",
    "#2007          19    19      19\n",
    "#2008          23    23      23\n",
    "#2009          15    15      15\n",
    "#2010          57    57      57\n",
    "#2011          95    95      95\n",
    "#2012          73    73      73\n",
    "#2013          96    96      96\n",
    "#2014         105   105     105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si necesitas comparar observaciones usando un solo parámetro, aplica el método al objeto DataFrameGroupBy e indica la columna en cuestión, por ejemplo 'radius' .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_number = exoplanet.groupby('discovered')['radius'].count()\n",
    "print(exo_number)\n",
    "print()\n",
    "print(type(exo_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discovered\n",
    "#1995      1\n",
    "#1996      1\n",
    "#1999      1\n",
    "#2000      2\n",
    "#2001      1\n",
    "#2002      1\n",
    "#2004      7\n",
    "#2005      4\n",
    "#2006     10\n",
    "#2007     19\n",
    "#2008     23\n",
    "#2009     15\n",
    "#2010     57\n",
    "#2011     95\n",
    "#2012     73\n",
    "#2013     96\n",
    "#2014    105\n",
    "#Name: radius, dtype: int64#\n",
    "\n",
    "#<class 'pandas.core.series.Series'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que agrupamos nuestros datos, podemos comprobar cómo, por ejemplo, ha cambiado con el tiempo el radio promedio de los exoplanetas descubiertos.\n",
    "\n",
    "Para calcular el promedio, encontraremos la suma de los radios de los exoplanetas descubiertos en un año determinado y la dividiremos entre el número de planetas, es decir, el valor que encontramos en el paso anterior.\n",
    "\n",
    "Primero, para encontrar la suma de los radios utiliza el método sum():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_radius_sum = exoplanet.groupby('discovered')['radius'].sum()\n",
    "print(exo_radius_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discovered\n",
    "#1995     1.900000\n",
    "#1996     1.060000\n",
    "#1999     1.380000\n",
    "#2000     2.007000\n",
    "#2001     0.921000\n",
    "#2002     1.200000\n",
    "#2004     6.789700\n",
    "#2005     4.789000\n",
    "#2006    20.355000\n",
    "#2007    24.334600\n",
    "#2008    31.329000\n",
    "#2009    15.366794\n",
    "#2010    56.828660\n",
    "#2011    77.738974\n",
    "#2012    50.074507\n",
    "#2013    69.372100\n",
    "#2014    55.268000\n",
    "#Name: radius, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, debemos dividirla entre la cantidad de exoplanetas descubiertos cada año. Los objetos Series se pueden dividir entre sí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_radius_mean = exo_radius_sum / exo_number\n",
    "print(exo_radius_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discovered\n",
    "#1995    1.900000\n",
    "#1996    1.060000\n",
    "#1999    1.380000\n",
    "#2000    1.003500\n",
    "#2001    0.921000\n",
    "#2002    1.200000\n",
    "#2004    0.969957\n",
    "#2005    1.197250\n",
    "#2006    2.035500\n",
    "#2007    1.280768\n",
    "#2008    1.362130\n",
    "#2009    1.024453\n",
    "#2010    0.996994\n",
    "#2011    0.818305\n",
    "#2012    0.685952\n",
    "#2013    0.722626\n",
    "#2014    0.526362\n",
    "#Name: radius, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "\n",
    "**Ejercicio 1**\n",
    "\n",
    "Revisemos nuestro conjunto de datos de música y agrupémoslo de manera similar a como lo hicimos con los exoplanetas. Es importante tener en cuenta que la agrupación generalmente se realiza en un dataset procesado que no contiene valores NaN, duplicados o nombres de columna sin formato. Por lo tanto, no usaremos el conjunto de datos music_log_raw.csv original, sino que usaremos el dataset preprocesado con todos los problemas eliminados.\n",
    "\n",
    "Nuestro primer paso es agrupar el conjunto de datos por 'genre'. Cuando se aplique el agrupamiento, guárdalo en la variable genre_groups y muestra su tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_processed.csv')\n",
    "\n",
    "genre_groups = df.groupby(\"genre\")\n",
    "\n",
    "print(type(genre_groups))\n",
    "#print(genre_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2**\n",
    "\n",
    "Ahora pasemos a la etapa de aplicación y apliquemos métodos de cálculo a cada grupo. Queremos calcular el tiempo total que nuestros oyentes pasaron escuchando cada género. Cuando hablamos del tiempo total, el método que debemos aplicar es sumar los valores de tiempo para cada género. Escríbelo en el precódigo a continuación y activa la variable genre_groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_processed.csv')\n",
    "\n",
    "genre_groups = df.groupby('genre').sum() # escribe tu código aquí\n",
    "\n",
    "print(genre_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3**\n",
    "\n",
    "Nuestro paso final es combinar los resultados. Por si acaso, te recordamos que queremos calcular el tiempo total que nuestros oyentes pasaron escuchando cada género. Tenemos una columna 'total_play' en nuestro dataset que contiene exactamente lo que necesitamos. Necesitamos pasar esto a nuestro flujo de agrupación: primero, selecciona la columna y luego aplica un método que calcule el tiempo total.\n",
    "Hazlo e imprime el resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_processed.csv')\n",
    "\n",
    "genre_groups = df.groupby('genre')[\"total_play\"].sum() # escribe tu código aquí\n",
    "\n",
    "print(genre_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ordenar datos**\n",
    "\n",
    "El método **sort_values()** en pandas es una poderosa herramienta para ordenar valores. Puede ordenar la tabla completa o grupos de filas dentro de ella.\n",
    "\n",
    "Este método se aplica a un DataFrame y tiene dos parámetros:\n",
    "\n",
    "- **by=**: el nombre o nombres de la columna cuyos valores se usan para ordenar las filas del DataFrame.\n",
    "- **ascending=** indica el orden al realizar el ordenamiento. Su valor predeterminado es True. Para ordenar los datos en orden descendente, establece este parámetro en False.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "```python\n",
    "exoplanet.sort_values(by=\"radius\",ascending=False)\n",
    "```\n",
    "donde: \n",
    "- exoplanet es el nombre de la tabla\n",
    "- radius ---> ordenar la columna\n",
    "- ascending ---> clave de ordenacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ordenar los datos de nuestros exoplanetas**\n",
    "\n",
    "Digamos que tenemos un gran interés en los exoplanetas que son similares en tamaño a la Tierra. Ordenemos los datos por radio en orden ascendente. Primero vendrán los planetas más pequeños:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exoplanet.sort_values(by='radius').head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "             name     mass    radius  discovered\n",
    "253   Kepler-37 b  0.01000  0.291431        2013\n",
    "137  Kepler-102 b  0.01353  0.470774        2014\n",
    "175  Kepler-138 b  0.00021  0.526818        2014\n",
    "327   Kepler-62 c  0.01300  0.538027        2013\n",
    "281   Kepler-42 d  0.00300  0.571654        2011\n",
    "138  Kepler-102 c  0.00944  0.582863        2014\n",
    "280   Kepler-42 c  0.00600  0.728579        2011\n",
    "254   Kepler-37 c  0.03776  0.750996        2013\n",
    "128      KOI-55 b  0.00140  0.762205        2011\n",
    "279   Kepler-42 b  0.00900  0.784623        2011\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el código que ordena únicamente la columna radius y muestra los primeros 10 resultados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exoplanet['radius'].sort_values().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "253    0.291431\n",
    "137    0.470774\n",
    "175    0.526818\n",
    "327    0.538027\n",
    "281    0.571654\n",
    "138    0.582863\n",
    "280    0.728579\n",
    "254    0.750996\n",
    "128    0.762205\n",
    "279    0.784623\n",
    "Name: radius, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que un valor de 1 significa que el radio es igual al de la Tierra. Parece que tenemos muchos exoplanetas que son más pequeños.\n",
    "\n",
    "Podemos recuperar todos los exoplanetas con radios más pequeños que el de la Tierra usando la indexación lógica:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print(exoplanet[exoplanet['radius'] < 1])\n",
    "\n",
    "             name     mass    radius  discovered\n",
    "128      KOI-55 b  0.00140  0.762205        2011\n",
    "129      KOI-55 c  0.00210  0.863085        2011\n",
    "137  Kepler-102 b  0.01353  0.470774        2014\n",
    "138  Kepler-102 c  0.00944  0.582863        2014\n",
    "141  Kepler-102 f  0.01636  0.885503        2014\n",
    "146  Kepler-106 b  0.01668  0.818250        2014\n",
    "148  Kepler-106 d  0.02549  0.952757        2014\n",
    "152  Kepler-107 d  0.01196  0.863085        2014\n",
    "174  Kepler-131 c  0.02600  0.840668        2014\n",
    "175  Kepler-138 b  0.00021  0.526818        2014\n",
    "194   Kepler-20 e  0.00970  0.863085        2011\n",
    "195   Kepler-20 f  0.04500  0.997592        2011\n",
    "253   Kepler-37 b  0.01000  0.291431        2013\n",
    "254   Kepler-37 c  0.03776  0.750996        2013\n",
    "264  Kepler-406 c  0.00900  0.851876        2014\n",
    "266  Kepler-408 b  0.01573  0.818250        2014\n",
    "279   Kepler-42 b  0.00900  0.784623        2011\n",
    "280   Kepler-42 c  0.00600  0.728579        2011\n",
    "281   Kepler-42 d  0.00300  0.571654        2011\n",
    "327   Kepler-62 c  0.01300  0.538027        2013\n",
    "336   Kepler-68 c  0.00642  0.926976        2013\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================\n",
    "Ahora, digamos que solo nos interesan los valores para el 2014, podemos nuevamente usar la indexación lógica para extraerlos:\n",
    "\n",
    "```python\n",
    "print(exoplanet[exoplanet['discovered'] == 2014])\n",
    "\n",
    "#Resultado\n",
    "\n",
    "            name    mass     radius  discovered\n",
    "42      GU Psc b  11.000  15.132015        2014\n",
    "84    HAT-P-49 b   1.730  15.838176        2014\n",
    "86    HAT-P-54 b   0.760  10.581202        2014\n",
    "92     HATS-15 b   2.170  12.385834        2014\n",
    "95      HATS-4 b   1.323  11.433078        2014\n",
    "..           ...     ...        ...         ...\n",
    "478    WASP-74 b   0.826  13.988707        2014\n",
    "487    WASP-83 b   0.300  11.657256        2014\n",
    "489  WASP-87 A b   2.210  15.524326        2014\n",
    "491    WASP-89 b   5.900  11.657256        2014\n",
    "493  WASP-94 A b   0.452  19.279308        2014\n",
    "\n",
    "[105 rows x 4 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Ordenemos el resultado por radio en orden descendente.\n",
    "\n",
    "```python\n",
    "print(exo_small_14.sort_values(by='radius', ascending=False))\n",
    "\n",
    "#Resultado\n",
    "\n",
    "             name     mass    radius  discovered\n",
    "148  Kepler-106 d  0.02549  0.952757        2014\n",
    "141  Kepler-102 f  0.01636  0.885503        2014\n",
    "152  Kepler-107 d  0.01196  0.863085        2014\n",
    "264  Kepler-406 c  0.00900  0.851876        2014\n",
    "174  Kepler-131 c  0.02600  0.840668        2014\n",
    "146  Kepler-106 b  0.01668  0.818250        2014\n",
    "266  Kepler-408 b  0.01573  0.818250        2014\n",
    "138  Kepler-102 c  0.00944  0.582863        2014\n",
    "175  Kepler-138 b  0.00021  0.526818        2014\n",
    "137  Kepler-102 b  0.01353  0.470774        2014\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método sort_values() devuelve un nuevo objeto en vez de modificarlo localmente. Por lo tanto, si deseas seguir trabajando con el DataFrame ordenado, deberás almacenar el resultado en una variable. Puedes guardarla nuevamente en la misma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_small_14 = exo_small_14.sort_values(by='radius', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "**Ejercicio 1**\n",
    "\n",
    "En la lección anterior, agrupaste nuestros datos music_log_processed.csv por 'genre' y calculaste el tiempo total que nuestros oyentes pasaron escuchando cada género. Como resultado, obtuvimos un objeto Series en el que tenemos el tiempo de escucha total para cada 'genre'. Ahora, ordenemos el objeto Series resultante en orden descendente y veamos los 10 géneros principales que nuestros oyentes escucharon más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/datasets/music_log_processed.csv')\n",
    "\n",
    "time_by_genre = df.groupby('genre')['total_play'].sum()\n",
    "\n",
    "time_by_genre_sort = time_by_genre.sort_values(ascending=False)\n",
    "\n",
    "print(time_by_genre_sort.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
