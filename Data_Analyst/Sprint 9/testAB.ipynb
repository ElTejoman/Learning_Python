{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test A/B\n",
    "\n",
    "### Ejercicios\n",
    "\n",
    "**Ejercicio 1**\n",
    "\n",
    "Un ejemplo de resultados de prueba A/B\n",
    "\n",
    "Encuentra el número total de visitantes y pedidos en cada grupo experimental. Almacena la tabla resultante en table y muestra esta variable. No cambies el precódigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/datasets/data_3-3.csv\", sep=\",\")\n",
    "\n",
    "print(data.head(5))\n",
    "\n",
    "table = (\n",
    "    data.drop([\"date\"], axis=1)\n",
    "    .groupby(\"group\", as_index=False)\n",
    "    .agg({\"visitors\": \"sum\", \"orders\": \"sum\"})\n",
    ")\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2**\n",
    "\n",
    "Encuentra la relación entre el número de pedidos y el número de visitantes para cada grupo experimental. Agrega los resultados a table, almacenándolos en la nueva columna orders_to_visitors_ratio.\n",
    "\n",
    "Redondea los valores a cuatro puntos decimales. Muestra table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/datasets/data_3-3.csv\", sep=\",\")\n",
    "\n",
    "print(data.head(5))\n",
    "\n",
    "table = (\n",
    "    data.drop([\"date\"], axis=1)\n",
    "    .groupby(\"group\", as_index=False)\n",
    "    .agg({\"visitors\": \"sum\", \"orders\": \"sum\"})\n",
    ")\n",
    "\n",
    "table[\"orders_to_visitors_ratio\"] = table[\"orders\"] / table[\"visitors\"]\n",
    "table[\"orders_to_visitors_ratio\"] = table[\"orders_to_visitors_ratio\"].round(4)\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo y por qué priorizamos las hipótesis\n",
    "\n",
    "#  **Ejercicio 1**\n",
    "\n",
    "Mira las hipótesis y los parámetros evaluados de WSJF en el archivo.\n",
    "\n",
    "Calcula el WSJF para las hipótesis. Agrega los valores de WSJF a la tabla existente. Asigna el nombre WSJF a la nueva columna. No redondees los valores.\n",
    "\n",
    "Muestra las columnas 'hypothesis' y 'WSJF' ordenadas por la columna 'WSJF' en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/datasets/hypothesis_4-1-1.csv\", sep=\",\")\n",
    "\n",
    "print(data)\n",
    "\n",
    "data[\"WSJF\"] = (\n",
    "    data[\"user_business_value\"]\n",
    "    + data[\"time_criticality\"]\n",
    "    + data[\"risk_reduction_opportunity_enablement\"]\n",
    ") / data[\"job_size\"]\n",
    "print(data[[\"hypothesis\", \"WSJF\"]].sort_values(by=\"WSJF\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2**\n",
    "\n",
    "Estudia las hipótesis y parámetros evaluados de ICE en el archivo.\n",
    "\n",
    "Calcula el ICE para las hipótesis. Agrega los valores obtenidos a la tabla existente. Asigna el nombre ICE a la nueva columna. No redondees los valores.\n",
    "\n",
    "Muestra las columnas 'hypothesis' y 'ICE' ordenadas por la columna 'ICE' en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/datasets/hypothesis_4-1-2.csv\", sep=\",\")\n",
    "\n",
    "print(data)\n",
    "\n",
    "data[\"ICE\"] = (data[\"impact\"] * data[\"confidence\"]) / data[\"efforts\"]\n",
    "print(data[[\"hypothesis\", \"ICE\"]].sort_values(by=\"ICE\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3**\n",
    "\n",
    "Calcula el RICE para las hipótesis. Agrega los valores obtenidos a la tabla existente. Asigna el nombre RICE a la nueva columna. No redondees los valores.\n",
    "\n",
    "Muestra las columnas 'hypothesis' y 'RICE' ordenadas por la columna 'RICE' en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/datasets/hypothesis_4-1-2.csv\", sep=\",\")\n",
    "\n",
    "print(data)\n",
    "\n",
    "data[\"RICE\"] = (data[\"reach\"] * data[\"impact\"] * data[\"confidence\"]) / data[\"efforts\"]\n",
    "print(data[[\"hypothesis\", \"RICE\"]].sort_values(by=\"RICE\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizar resultados de Test A/B\n",
    "\n",
    "### Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una empresa ofreció a 400 de 900 clientes potenciales (usuarios que mostraron algún interés en un producto) una oferta especial para servicios ampliados. A los 500 usuarios restantes se les ofreció el paquete de servicios original.\n",
    "\n",
    "En cada grupo, 100 usuarios pidieron el paquete.\n",
    "\n",
    "¿Funcionó la promoción? Prueba la hipótesis de que las proporciones de los pedidos en estos dos segmentos de clientes potenciales eran iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as st\n",
    "import numpy as np\n",
    "import math as mth\n",
    "\n",
    "alpha = 0.05  # nivel de significación\n",
    "\n",
    "purchases = np.array([100, 100])\n",
    "leads = np.array([400, 500])\n",
    "\n",
    "p1 = purchases[0] / leads[0]\n",
    "\n",
    "p2 = purchases[1] / leads[1]\n",
    "\n",
    "p_combined = (purchases[0] + purchases[1]) / (leads[0] + leads[1])\n",
    "\n",
    "difference = p1 - p2\n",
    "\n",
    "z_value = difference / mth.sqrt(\n",
    "    p_combined * (1 - p_combined) * (1 / leads[0] + 1 / leads[1])\n",
    ")\n",
    "\n",
    "distr = st.norm(0, 1)\n",
    "\n",
    "p_value = (1 - distr.cdf(abs(z_value))) * 2\n",
    "\n",
    "print(\"p-value: \", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\n",
    "        \"Rechazar la hipótesis nula: hay una diferencia significativa entre las proporciones\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No se pudo rechazar la hipótesis nula: no hay razón para pensar que las proporciones son diferentes\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El test Shapito - Wilk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una muestra con recibos de la tienda en línea redondeados al dólar se almacena en sales_data. Comprueba si los totales de estos recibos se distribuyen normalmente utilizando el test de Shapiro-Wilk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as st\n",
    "\n",
    "sales_data = [\n",
    "    324,\n",
    "    209,\n",
    "    217,\n",
    "    321,\n",
    "    210,\n",
    "    231,\n",
    "    235,\n",
    "    519,\n",
    "    210,\n",
    "    240,\n",
    "    213,\n",
    "    325,\n",
    "    252,\n",
    "    251,\n",
    "    246,\n",
    "    353,\n",
    "    260,\n",
    "    256,\n",
    "    203,\n",
    "    212,\n",
    "    211,\n",
    "    318,\n",
    "    529,\n",
    "    252,\n",
    "    227,\n",
    "    278,\n",
    "    221,\n",
    "    222,\n",
    "    257,\n",
    "    289,\n",
    "    208,\n",
    "    256,\n",
    "    308,\n",
    "    395,\n",
    "    485,\n",
    "    350,\n",
    "    214,\n",
    "    378,\n",
    "    218,\n",
    "    261,\n",
    "    216,\n",
    "    289,\n",
    "    533,\n",
    "    239,\n",
    "    326,\n",
    "    445,\n",
    "    210,\n",
    "    284,\n",
    "    317,\n",
    "    260,\n",
    "    420,\n",
    "    497,\n",
    "    321,\n",
    "    205,\n",
    "    237,\n",
    "    261,\n",
    "    205,\n",
    "    269,\n",
    "    246,\n",
    "    685,\n",
    "    246,\n",
    "    207,\n",
    "    317,\n",
    "    236,\n",
    "    519,\n",
    "    230,\n",
    "    208,\n",
    "    202,\n",
    "    216,\n",
    "    234,\n",
    "    242,\n",
    "    200,\n",
    "    226,\n",
    "    213,\n",
    "    440,\n",
    "    1026,\n",
    "    318,\n",
    "    286,\n",
    "    210,\n",
    "    216,\n",
    "    227,\n",
    "    256,\n",
    "    221,\n",
    "    216,\n",
    "    204,\n",
    "    498,\n",
    "    223,\n",
    "    287,\n",
    "    296,\n",
    "    292,\n",
    "    406,\n",
    "    213,\n",
    "    210,\n",
    "    291,\n",
    "    217,\n",
    "    200,\n",
    "    344,\n",
    "    296,\n",
    "    222,\n",
    "    258,\n",
    "    223,\n",
    "    422,\n",
    "    497,\n",
    "    325,\n",
    "    328,\n",
    "    201,\n",
    "    242,\n",
    "    255,\n",
    "    203,\n",
    "    252,\n",
    "    254,\n",
    "    221,\n",
    "    527,\n",
    "    231,\n",
    "    506,\n",
    "    203,\n",
    "    261,\n",
    "    678,\n",
    "    209,\n",
    "    261,\n",
    "    281,\n",
    "    210,\n",
    "    292,\n",
    "    354,\n",
    "    210,\n",
    "    235,\n",
    "    220,\n",
    "    204,\n",
    "    270,\n",
    "    218,\n",
    "    230,\n",
    "    295,\n",
    "    215,\n",
    "    372,\n",
    "    218,\n",
    "    230,\n",
    "    282,\n",
    "    284,\n",
    "    229,\n",
    "    210,\n",
    "    206,\n",
    "    267,\n",
    "    299,\n",
    "    263,\n",
    "    563,\n",
    "    215,\n",
    "    258,\n",
    "    214,\n",
    "    351,\n",
    "    201,\n",
    "]\n",
    "\n",
    "alpha = 0.05  # nivel de significación\n",
    "\n",
    "results = st.shapiro(sales_data)\n",
    "p_value = results[\n",
    "    1\n",
    "]  # el segundo valor en la matriz de resultados (con índice 1) - el valor p\n",
    "\n",
    "print(\"p-value: \", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Hipótesis nula rechazada: la distribución no es normal\")\n",
    "else:\n",
    "    print(\"No se pudo rechazar la hipótesis nula: la distribución parece ser normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La prueba no paramétrica de Wilcoxon-Mann-Whitney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tienes datos sobre el tamaño de compra promedio antes y después de que se introdujera un programa de lealtad para aquellos que realizan compras grandes: esos clientes recibieron mayor cashback de un banco famoso. Revisa las muestras para ver si es posible decir que el programa fue un éxito.\n",
    "\n",
    "Para completar esta tarea, debes probar una hipótesis unilateral. Además de los datasets, pasarás al método otros dos parámetros.\n",
    "\n",
    "El tercer parámetro es de tipo booleano y define si debes corregir la continuidad de la distribución que se usa para describir datos discretos en la prueba. Hay que tenerlo en cuenta pero no afecta tanto el resultado. Este parámetro es True por defecto.  Si pretendes establecer el cuarto parámetro, el parámetro booleano debe ser True. El cuarto parámetro se escribe como string: 'less', 'two-sided' o 'greater'. Así es como comparamos el primer dataset pasado al método con el segundo. En esta tarea, tenemos que verificar si el segundo dataset es más grande y el primero es más pequeño por lo que estableceremos el parámetro como 'less'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as st\n",
    "\n",
    "sales_before = [\n",
    "    25939,\n",
    "    14569,\n",
    "    15040,\n",
    "    28317,\n",
    "    21100,\n",
    "    13597,\n",
    "    62869,\n",
    "    46195,\n",
    "    13414,\n",
    "    13928,\n",
    "    17136,\n",
    "    14729,\n",
    "    25754,\n",
    "    17254,\n",
    "    16628,\n",
    "    16605,\n",
    "    40711,\n",
    "    74209,\n",
    "    14498,\n",
    "    32265,\n",
    "    13873,\n",
    "    16724,\n",
    "    22522,\n",
    "    14824,\n",
    "    21825,\n",
    "    32522,\n",
    "    14485,\n",
    "    16779,\n",
    "    17574,\n",
    "    16772,\n",
    "    18331,\n",
    "    19170,\n",
    "    13753,\n",
    "    15551,\n",
    "    17202,\n",
    "    13725,\n",
    "    15415,\n",
    "    16155,\n",
    "    49620,\n",
    "    33900,\n",
    "    23834,\n",
    "    25732,\n",
    "    16539,\n",
    "    24449,\n",
    "    14681,\n",
    "    15000,\n",
    "    14521,\n",
    "    13298,\n",
    "    14421,\n",
    "    17500,\n",
    "    15949,\n",
    "    16246,\n",
    "    19259,\n",
    "    15283,\n",
    "    14418,\n",
    "    18026,\n",
    "    25931,\n",
    "    14182,\n",
    "    13837,\n",
    "    23061,\n",
    "    14074,\n",
    "    25344,\n",
    "    19134,\n",
    "    14177,\n",
    "    19357,\n",
    "    96794,\n",
    "    26358,\n",
    "    16599,\n",
    "    15426,\n",
    "    23417,\n",
    "    68856,\n",
    "    44375,\n",
    "    14669,\n",
    "    39750,\n",
    "    34531,\n",
    "    14655,\n",
    "    28580,\n",
    "    25176,\n",
    "    55065,\n",
    "    64288,\n",
    "    16069,\n",
    "    16745,\n",
    "    13548,\n",
    "    19177,\n",
    "    19173,\n",
    "    16473,\n",
    "    15534,\n",
    "    20115,\n",
    "    16608,\n",
    "    15261,\n",
    "    13472,\n",
    "    47956,\n",
    "    21036,\n",
    "    19238,\n",
    "    25955,\n",
    "    14755,\n",
    "    16901,\n",
    "    13740,\n",
    "    13585,\n",
    "    23080,\n",
    "    17259,\n",
    "    51311,\n",
    "    47505,\n",
    "    19582,\n",
    "    13968,\n",
    "    46805,\n",
    "    14261,\n",
    "    18376,\n",
    "    13314,\n",
    "    37948,\n",
    "    18404,\n",
    "    16911,\n",
    "    18692,\n",
    "    19885,\n",
    "    16619,\n",
    "    15234,\n",
    "    21832,\n",
    "    228535,\n",
    "    28377,\n",
    "    16452,\n",
    "    13293,\n",
    "    17915,\n",
    "    15527,\n",
    "    17671,\n",
    "    24046,\n",
    "    15645,\n",
    "    14350,\n",
    "    16765,\n",
    "    17600,\n",
    "    14222,\n",
    "    25300,\n",
    "    16941,\n",
    "    14758,\n",
    "    17120,\n",
    "    14621,\n",
    "    25596,\n",
    "    20472,\n",
    "    24871,\n",
    "    14504,\n",
    "    17956,\n",
    "    20565,\n",
    "    18868,\n",
    "    16980,\n",
    "    40395,\n",
    "    13868,\n",
    "    14572,\n",
    "    13893,\n",
    "    17986,\n",
    "    14490,\n",
    "    16891,\n",
    "]\n",
    "sales_after = [\n",
    "    17484,\n",
    "    18369,\n",
    "    19412,\n",
    "    35496,\n",
    "    30841,\n",
    "    18511,\n",
    "    16438,\n",
    "    16064,\n",
    "    27841,\n",
    "    18335,\n",
    "    20978,\n",
    "    18266,\n",
    "    24675,\n",
    "    16355,\n",
    "    15245,\n",
    "    14960,\n",
    "    15448,\n",
    "    14181,\n",
    "    20095,\n",
    "    15586,\n",
    "    18594,\n",
    "    14414,\n",
    "    50452,\n",
    "    18804,\n",
    "    16750,\n",
    "    17313,\n",
    "    20047,\n",
    "    25674,\n",
    "    30803,\n",
    "    14567,\n",
    "    16871,\n",
    "    17667,\n",
    "    48241,\n",
    "    15191,\n",
    "    135885,\n",
    "    104794,\n",
    "    18650,\n",
    "    16708,\n",
    "    26201,\n",
    "    15926,\n",
    "    40253,\n",
    "    17787,\n",
    "    28374,\n",
    "    22989,\n",
    "    21122,\n",
    "    14938,\n",
    "    115634,\n",
    "    18351,\n",
    "    15895,\n",
    "    14951,\n",
    "    15177,\n",
    "    25709,\n",
    "    76209,\n",
    "    99617,\n",
    "    16452,\n",
    "    16446,\n",
    "    19407,\n",
    "    21144,\n",
    "    14947,\n",
    "    26257,\n",
    "    23723,\n",
    "    18113,\n",
    "    27784,\n",
    "    38882,\n",
    "    15907,\n",
    "    15741,\n",
    "    21705,\n",
    "    32604,\n",
    "    16101,\n",
    "    17870,\n",
    "    15794,\n",
    "    18423,\n",
    "    18381,\n",
    "    194987,\n",
    "    15335,\n",
    "    14022,\n",
    "    21257,\n",
    "    29935,\n",
    "    14598,\n",
    "    26066,\n",
    "    47228,\n",
    "    37022,\n",
    "    15071,\n",
    "    21353,\n",
    "    38690,\n",
    "    40838,\n",
    "    26125,\n",
    "    24722,\n",
    "    30756,\n",
    "    17099,\n",
    "    21377,\n",
    "    14611,\n",
    "    44442,\n",
    "    15808,\n",
    "    17173,\n",
    "    93187,\n",
    "    30411,\n",
    "    15279,\n",
    "    25707,\n",
    "    35374,\n",
    "    70792,\n",
    "    14918,\n",
    "    21678,\n",
    "    16453,\n",
    "    40998,\n",
    "    27836,\n",
    "    18411,\n",
    "    46965,\n",
    "    15968,\n",
    "    22812,\n",
    "    15856,\n",
    "    17933,\n",
    "    23682,\n",
    "    33450,\n",
    "    21727,\n",
    "    17884,\n",
    "    21676,\n",
    "    124684,\n",
    "    20145,\n",
    "    16041,\n",
    "    14872,\n",
    "    17588,\n",
    "    17436,\n",
    "    81993,\n",
    "    20497,\n",
    "    17484,\n",
    "    58826,\n",
    "    26179,\n",
    "    17515,\n",
    "    27463,\n",
    "    14260,\n",
    "    27331,\n",
    "    17598,\n",
    "    41888,\n",
    "    14037,\n",
    "    15517,\n",
    "    19704,\n",
    "    16718,\n",
    "    32514,\n",
    "    38851,\n",
    "    18925,\n",
    "    23982,\n",
    "    14104,\n",
    "    21690,\n",
    "    60266,\n",
    "    21071,\n",
    "    42799,\n",
    "    16203,\n",
    "    16694,\n",
    "    22699,\n",
    "]\n",
    "\n",
    "alpha = 0.05  # el nivel de significancia estadística crítica\n",
    "\n",
    "results = st.mannwhitneyu(sales_before, sales_after, True, \"less\")\n",
    "\n",
    "print(\"valor p: \", results.pvalue)\n",
    "\n",
    "if results.pvalue < alpha:\n",
    "    print(\n",
    "        \"Hipótesis nula rechazada: la primera serie es significativamente más pequeña que la segunda\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No se rechazó la hipótesis nula: no es posible concluir que la primera serie sea menor que la segunda\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estabilidad de las métricas acumuladas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Estudia los datos de los archivos con pedidos y visitantes.\n",
    "\n",
    "La tabla de pedidos contiene las siguientes columnas:\n",
    "\n",
    "- orderId\n",
    "- userId\n",
    "- group\n",
    "- revenue\n",
    "- date\n",
    "- La tabla de visitantes contiene las siguientes columnas:\n",
    "\n",
    "- date\n",
    "- group\n",
    "- visitors — el número de visitantes en la fecha especificada para el grupo especificado\n",
    "1)  Crea un DataFrame llamado datesGroups con parejas de valores únicos de 'date' y 'group' de la tabla orders. Deshazte de los valores duplicados con el método drop_duplicates().\n",
    "\n",
    "2) Declara la variable ordersAggregated para almacenar:\n",
    "\n",
    "- la fecha\n",
    "- el grupo del test A/B\n",
    "- el número de pedidos distintos para el grupo de prueba hasta la fecha especificada incluida\n",
    "- el número de usuarios distintos en el grupo de prueba que realizan al menos un pedido hasta la fecha especificada incluida\n",
    "- ingresos totales de pedidos en el grupo de prueba hasta la fecha especificada incluida\n",
    "3) Declara la variable visitorsAggregated para almacenar:\n",
    "\n",
    "- la fecha\n",
    "- el grupo del test A/B\n",
    "- el número de pedidos distintos para el grupo de prueba hasta la fecha especificada incluida\n",
    "4) Ordena ordersAggregated y visitorsAggregated por las columnas 'date' y 'group', en ese orden.\n",
    "\n",
    "5) Define la variable cumulativeData uniendo ordersAggregated y visitorsAggregated por las columnas 'date' y 'group' con el método merge().\n",
    "\n",
    "6) Asigna los nombres ['date', 'group', 'orders', 'buyers', 'revenue', 'visitors'] a las columnas en cumulativeData.\n",
    "\n",
    "7) Imprime las primeras cinco filas de cumulativeData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "orders = pd.read_csv(\"/datasets/data_for_tasks_3.csv\", sep=\",\")\n",
    "orders[\"date\"] = orders[\"date\"].map(lambda x: dt.datetime.strptime(x, \"%d/%m/%Y\"))\n",
    "\n",
    "visitors = pd.read_csv(\"/datasets/data_for_tasks_3_visitors.csv\", sep=\",\")\n",
    "visitors[\"date\"] = visitors[\"date\"].map(lambda x: dt.datetime.strptime(x, \"%d/%m/%Y\"))\n",
    "\n",
    "print(orders.head(5))\n",
    "print(visitors.head(5))\n",
    "\n",
    "datesGroups = orders[[\"date\", \"group\"]].drop_duplicates()\n",
    "\n",
    "ordersAggregated = datesGroups.apply(\n",
    "    lambda x: orders[\n",
    "        np.logical_and(orders[\"date\"] <= x[\"date\"], orders[\"group\"] == x[\"group\"])\n",
    "    ].agg(\n",
    "        {\n",
    "            \"date\": \"max\",\n",
    "            \"group\": \"max\",\n",
    "            \"orderId\": pd.Series.nunique,\n",
    "            \"userId\": pd.Series.nunique,\n",
    "            \"revenue\": \"sum\",\n",
    "        }\n",
    "    ),\n",
    "    axis=1,\n",
    ").sort_values(by=[\"date\", \"group\"])\n",
    "\n",
    "visitorsAggregated = datesGroups.apply(\n",
    "    lambda x: visitors[\n",
    "        np.logical_and(visitors[\"date\"] <= x[\"date\"], visitors[\"group\"] == x[\"group\"])\n",
    "    ].agg({\"date\": \"max\", \"group\": \"max\", \"visitors\": \"sum\"}),\n",
    "    axis=1,\n",
    ").sort_values(by=[\"date\", \"group\"])\n",
    "\n",
    "cumulativeData = ordersAggregated.merge(\n",
    "    visitorsAggregated, left_on=[\"date\", \"group\"], right_on=[\"date\", \"group\"]\n",
    ")\n",
    "cumulativeData.columns = [\n",
    "    \"date\",\n",
    "    \"group\",\n",
    "    \"orders\",\n",
    "    \"buyers\",\n",
    "    \"revenue\",\n",
    "    \"visitors\",\n",
    "]\n",
    "\n",
    "print(cumulativeData.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Declara las variables cumulativeRevenueA y cumulativeRevenueB, donde almacenarás los datos sobre fechas, ingresos y número de pedidos para los grupos A y B.\n",
    "\n",
    "Traza gráficos de ingresos acumulados diarios para cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "orders = pd.read_csv(\"/datasets/data_for_tasks_3.csv\", sep=\",\")\n",
    "orders[\"date\"] = orders[\"date\"].map(lambda x: dt.datetime.strptime(x, \"%d/%m/%Y\"))\n",
    "\n",
    "visitors = pd.read_csv(\"/datasets/data_for_tasks_3_visitors.csv\", sep=\",\")\n",
    "visitors[\"date\"] = visitors[\"date\"].map(lambda x: dt.datetime.strptime(x, \"%d/%m/%Y\"))\n",
    "\n",
    "datesGroups = orders[[\"date\", \"group\"]].drop_duplicates()\n",
    "\n",
    "ordersAggregated = datesGroups.apply(\n",
    "    lambda x: orders[\n",
    "        np.logical_and(orders[\"date\"] <= x[\"date\"], orders[\"group\"] == x[\"group\"])\n",
    "    ].agg(\n",
    "        {\n",
    "            \"date\": \"max\",\n",
    "            \"group\": \"max\",\n",
    "            \"orderId\": pd.Series.nunique,\n",
    "            \"userId\": pd.Series.nunique,\n",
    "            \"revenue\": \"sum\",\n",
    "        }\n",
    "    ),\n",
    "    axis=1,\n",
    ").sort_values(by=[\"date\", \"group\"])\n",
    "\n",
    "visitorsAggregated = datesGroups.apply(\n",
    "    lambda x: visitors[\n",
    "        np.logical_and(visitors[\"date\"] <= x[\"date\"], visitors[\"group\"] == x[\"group\"])\n",
    "    ].agg({\"date\": \"max\", \"group\": \"max\", \"visitors\": \"sum\"}),\n",
    "    axis=1,\n",
    ").sort_values(by=[\"date\", \"group\"])\n",
    "\n",
    "cumulativeData = ordersAggregated.merge(\n",
    "    visitorsAggregated, left_on=[\"date\", \"group\"], right_on=[\"date\", \"group\"]\n",
    ")\n",
    "cumulativeData.columns = [\n",
    "    \"date\",\n",
    "    \"group\",\n",
    "    \"orders\",\n",
    "    \"buyers\",\n",
    "    \"revenue\",\n",
    "    \"visitors\",\n",
    "]\n",
    "\n",
    "cumulativeRevenueA = cumulativeData[cumulativeData[\"group\"] == \"A\"][\n",
    "    [\"date\", \"revenue\", \"orders\"]\n",
    "]\n",
    "cumulativeRevenueB = cumulativeData[cumulativeData[\"group\"] == \"B\"][\n",
    "    [\"date\", \"revenue\", \"orders\"]\n",
    "]\n",
    "\n",
    "plt.plot(cumulativeRevenueA[\"date\"], cumulativeRevenueA[\"revenue\"], label=\"A\")\n",
    "plt.plot(cumulativeRevenueB[\"date\"], cumulativeRevenueB[\"revenue\"], label=\"B\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has mostrado exitosamente con un gráfico la diferencia relativa en el tamaño de compra promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "orders = pd.read_csv(\"/datasets/data_for_tasks_3.csv\", sep=\",\")\n",
    "orders[\"date\"] = orders[\"date\"].map(lambda x: dt.datetime.strptime(x, \"%d/%m/%Y\"))\n",
    "\n",
    "visitors = pd.read_csv(\"/datasets/data_for_tasks_3_visitors.csv\", sep=\",\")\n",
    "visitors[\"date\"] = visitors[\"date\"].map(lambda x: dt.datetime.strptime(x, \"%d/%m/%Y\"))\n",
    "\n",
    "datesGroups = orders[[\"date\", \"group\"]].drop_duplicates()\n",
    "\n",
    "ordersAggregated = datesGroups.apply(\n",
    "    lambda x: orders[\n",
    "        np.logical_and(orders[\"date\"] <= x[\"date\"], orders[\"group\"] == x[\"group\"])\n",
    "    ].agg(\n",
    "        {\n",
    "            \"date\": \"max\",\n",
    "            \"group\": \"max\",\n",
    "            \"orderId\": pd.Series.nunique,\n",
    "            \"userId\": pd.Series.nunique,\n",
    "            \"revenue\": \"sum\",\n",
    "        }\n",
    "    ),\n",
    "    axis=1,\n",
    ").sort_values(by=[\"date\", \"group\"])\n",
    "\n",
    "visitorsAggregated = datesGroups.apply(\n",
    "    lambda x: visitors[\n",
    "        np.logical_and(visitors[\"date\"] <= x[\"date\"], visitors[\"group\"] == x[\"group\"])\n",
    "    ].agg({\"date\": \"max\", \"group\": \"max\", \"visitors\": \"sum\"}),\n",
    "    axis=1,\n",
    ").sort_values(by=[\"date\", \"group\"])\n",
    "\n",
    "cumulativeData = ordersAggregated.merge(\n",
    "    visitorsAggregated, left_on=[\"date\", \"group\"], right_on=[\"date\", \"group\"]\n",
    ")\n",
    "cumulativeData.columns = [\n",
    "    \"date\",\n",
    "    \"group\",\n",
    "    \"orders\",\n",
    "    \"buyers\",\n",
    "    \"revenue\",\n",
    "    \"visitors\",\n",
    "]\n",
    "\n",
    "cumulativeRevenueA = cumulativeData[cumulativeData[\"group\"] == \"A\"][\n",
    "    [\"date\", \"revenue\", \"orders\"]\n",
    "]\n",
    "cumulativeRevenueB = cumulativeData[cumulativeData[\"group\"] == \"B\"][\n",
    "    [\"date\", \"revenue\", \"orders\"]\n",
    "]\n",
    "\n",
    "plt.plot(\n",
    "    cumulativeRevenueA[\"date\"],\n",
    "    cumulativeRevenueA[\"revenue\"] / cumulativeRevenueA[\"orders\"],\n",
    "    label=\"A\",\n",
    ")\n",
    "plt.plot(\n",
    "    cumulativeRevenueB[\"date\"],\n",
    "    cumulativeRevenueB[\"revenue\"] / cumulativeRevenueB[\"orders\"],\n",
    "    label=\"B\",\n",
    ")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
